# Continue to learn https://martinfowler.com/articles/cd4ml.html 


## Case study ``sales forcasting``

Original problem is from https://www.kaggle.com/c/favorita-grocery-sales-forecasting/overview 

## Common Challenge
1. Multiple team and stack holder 
2. How to make sure the process is reproducible 


# Technical component for CD4ML
## Data discovery and access
Data must be easily accessable by DS and engineer in datalake. 

## Reproducible ML training
Used a open source tools called DVC. 
The overall stages are: (assume features is accessable from datalake)
1. Load in ML data, (This step already have certain tracking mechanism to make sure the input data been tracked)
2. Split data into training and validation set
3. run ML training


## Model Serving
1. Embedded model 
The ML model is the depending of the serving code. Version would be the combination of code and model artifact 
Mleap provide a common serialized format for many ML models 

2. Model deployed as a separated server 
E.g. Tensorflow serve 

3. Model publish as data 

## Testing
1. Data Validation 
Type checking, range checking, other type of sanity checking 

2. Validation component 
Validation of model contract, input output format, 
Validate of model against existing model for the same validation set 

3. Validation of model quality
Error rate, AUC, ROC, confusion matrix, precision & recall

4. Validation of model bias and fairness 
Cross validation 

A holdout dataset together with your model release is important for your model test and reaccess model performance. 


## Experience Tracking

DVC use version control to track different branch to track different experience in source control. 
The downside of this approach is multiple long lived branch will cause a merge pain. 

The other possible way is ml-flow, 

## Model deployment
1. Shadow model  
 * Before replace the current model of production, the new model can be deployed side by side and evaluate shadow 
 performance before roll out to production. 
2. Competing model
A/B testing for deployment. The routing rule is required to direct traffic to one of the model. 
* Multi-Armed Bandits approach for model evaluation. There are some tools can help you with that like Seldon core, 
or Azure Personalizer  
3. Online learning model 
Online learning model will continue improvement itself with online data as the time passed by.

## Continuous Delivery Orchestration
We need to ochestration platform to eventually link everything together. 
From machine learning model test, to training and validation, we need a platform to eventually tight things together. 

Example for such platform would be (gocd)[https://www.gocd.org/]  
Two kinds of pipeline:
1. Machine learning pipeline:
* Training 
* Experience tracking   
* Model testing 
* Hyper parameter turning 

2. Model serving pipeline:
* Build and test app code
* Fetch model and deploy 
* Roll back production as well 

## Model monitoring and Observability

Gather model metric in production and close the loop
There are following metrics we need to consider
1. Model inputs
2. Model interpretability outputs:
    * coefficients
    * (eli5)[https://eli5.readthedocs.io/en/latest]
    * (LIME)[https://arxiv.org/abs/1602.04938]
3. Model outputs
4. User further actions and rewards 








 