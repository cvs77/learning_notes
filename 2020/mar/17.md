# Pipeline Quick Starts
https://www.kubeflow.org/docs/pipelines/pipelines-quickstart/

## Concepts
### Pipeline 
A description of machine learning pipline. Include all component and how components are related in graph. 
The pipeline configuration include the input of pipeline and input output for each component. 

The pipeline is shipped by pipeline UI or python SDK

### components
#### code
The code is separated as client code and runtime code
* client code is the part executed by argo like submit spark job job
* the runtime code is like spark code running in cluster

#### Component definition
A component have following parts:
1. metadata
2. interface: input/output specification (name, type, description, default value)
3. Implementation: A specification of how to run the component given a set of argument values for the component’s inputs.
 The implementation section also describes how to get the output values from the component once the component has finished running.
 Details: https://www.kubeflow.org/docs/pipelines/reference/component-spec/
 
In kubeflow, container have to run with docker images. 

#### Experiment
Each pipeline run will become a experiment, including recurring runs

#### Run trigger
Two type of trigger
* Periodic: for an interval-based scheduling of runs (for example: every 2 hours or every 45 minutes).
* cron: for specifying cron semantics for scheduling runs.

#### Step 
Step is like a instantiation of a step 
 In a complex pipeline, components can execute multiple times in loops, or conditionally after resolving an if/else like clause in the pipeline code 
 
#### Output Artifact 
It is the output emitted by a pipeline, it can be a plain text or a format kubeflow pipeline can render


## Examples
### Compile with command line took
1. Kubeflow samle is here https://github.com/kubeflow/pipelines/tree/master/samples
2. Install kubeflow SDK: pip3 install kfp 
* set up dsl-compile 
This command installs the dsl-compile and kfp binaries under ~/.local/bin, which is not part of the PATH in some Linux distributions, such as Ubuntu. You can add ~/.local/bin to your PATH by appending the following to a new line at the end of your .bashrc file:

                    export PATH=$PATH:~/.local/bin
3. Use dsl-compile to generate static yaml file from python code


                    dsl-compile --py [path/to/python/file] --output [path/to/output/tar.gz]
4. Upload piplefile through kubeflow UI

### build from jupyter notebook

* Tensorflow example: https://github.com/kubeflow/pipelines/blob/master/samples/core/tfx-oss/TFX%20Example.ipynb
* Simple python code example:https://github.com/kubeflow/pipelines/blob/master/samples/core/lightweight_component/lightweight_component.ipynb
More example for check: https://github.com/kubeflow/pipelines/tree/master/samples/core

### Pipeline SDK

Pipeline SDK is a set of decoration for you to generate static yaml file for you to run in pipeline
From your existing code, the pipeline is generated though layer of layer decoration, eventually preset as a pipeline
then use 
```python
kfp.compiler.Compiler().compile(my_pipeline,  
  'my-pipeline.zip')
```
to compile to yaml file
Step by step tutorial is here https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/#creating-components-from-existing-application-code

For python functions, you can convert it to a pipeline component as well
```python
 kfp.components.func_to_container_op t()
```

## Build component from docker file
[link to doc](https://www.kubeflow.org/docs/pipelines/sdk/component-development/)
It is similar with chimeracli sandbox submit
Require you to build docker -> define docker image and comand -> create component from yaml file with following command 
```python
kfp.components.load_component_from_url('http://....../component.yaml')
```

## Light weight python component
For this light weight pythoh component, usually rebuild docker is not needed. 
There is following requirement of such python function:
* The function must be stand-alone
* The function can only import packages that are available in the base image.
* If the function operates on numbers, the parameters must have type hints
* To build a component with multiple output values, use Python’s typing.NamedTuple type hint syntax:

## best practise
For writing component [link](https://www.kubeflow.org/docs/pipelines/sdk/best-practices/)
For writing docker [link](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)

## pipeline parameters
Pipeline parameters is a reference of future data which gonna passed into task.It can be set from UI
It can represented as a intermediate value, or referrence in output. 
