# Kubeflow Installation and Pipeline in Depth

## installation. 
This Kubeflow deployment requires a default StorageClass with a dynamic volume provisioner

Kubeflow is install by kfctl apply, you can choose kfctl build to set up configuraiton for later deployment. 

## kubeflow pipeline

### Install
standard kubectl apply https://www.kubeflow.org/docs/pipelines/installation/standalone-deployment/#deploying-kubeflow-pipelines

For customized build, you can use overlay function provided in kustomize https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#overlay

### Overview
#### PythonSDK
allow create kubeflow pipeline through python. 

#### DSL compiler 
eventually help you compile your python code to a static configuration in YAML format. 

####Pipeline
service was called to create pipeline froms static configuration. It call k8s services to create CRD. 

#### Orchestration Controller
Then CRD is handel by Orchestration Controller here is Argo controller for the most of cased in kubeflow context. 

#### Artifact Store
Then a important concept came in which is Artifact Storage. There are two kind of data a pod is needed. 

* metadata: experience, job, pipelines run, and single scalar metric. Metric data is aggregated for the purpose of filtering and sorting.
It is stored in metadata. 
* Artifacts: Pipeline packages, views, and large-scale metrics (time series). Use large-scale metrics to debug a 
pipeline run or investigate an individual runâ€™s performance. Kubeflow Pipelines stores the artifacts in an artifact 
store like Minio server 

#### Persistent Agent
The pieple persistent agent watch k8s resources created by pipeline services and persis this 
resources in ML metasserver

#### Pipeline webserver
Gather data and display, current execution, historical execution and so on. 